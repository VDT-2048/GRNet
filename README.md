# GRNet
We propose a cross-modality salient object detection network with universality and anti-interference, i.e., GRNet. First, we offer a feature extraction strategy to enhance the features in the feature extraction stage. It can promote the mutual improvement of different modal information and avoid the influence of interference on the subsequent process. Then we use the graph mapping reasoning module (GMRM) to infer the high-level semantics to obtain valuable information. It enables our proposed method to accurately locate the objects in different scenes and interference to improve the universality and anti-interference of the method. Finally, we adopt a mutual guidance fusion module (MGFM), including a modality adaptive fusion module (MAFM) and across-level mutual guidance fusion module (ALMGFM), to carry out an efficient and reasonable fusion of multi-scale and multi-modality information. To verify the universality and anti-interference of our proposed method, we conduct experiments on many RGB-D/T SOD datasets and compare our method with the current state-of-the-art methods. Experimental results show that our method performs well in universality and anti-interference.

![GRNet](https://user-images.githubusercontent.com/101933818/214591717-270e8122-78d4-4e4e-89ce-ec1c1b3aef5d.jpg)


# Download the code
The code is available at:https://pan.baidu.com/s/1kgoF_QgwAHW6kOEnGRfDAw?pwd=0prp 

# Paper
https://www.sciencedirect.com/science/article/abs/pii/S0950705123000722

# Citation
Wen H, Song K, et al.Cross-modality salient object detection network with universality and anti-interference[J]. Knowledge-Based Systems, 2023, 264: 110322.
